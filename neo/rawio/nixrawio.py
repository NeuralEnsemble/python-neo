"""
RawIO Class for NIX files

The RawIO assumes all segments and all blocks have the same structure.
It supports all kinds of NEO objects.

Author: Chek Yin Choi, Julia Sprenger
"""
import warnings

import numpy as np
from packaging.version import Version

from .baserawio import (BaseRawIO, _signal_channel_dtype, _signal_stream_dtype,
                        _spike_channel_dtype, _event_channel_dtype)
from ..io.nixio import check_nix_version

# When reading metadata properties, the following keys are ignored since they
# are used to store Neo object properties.
# This dictionary is used in the _filter_properties() method.
neo_attributes = {
    "segment": ["index"],
    "analogsignal": ["units", "copy", "sampling_rate", "t_start"],
    "spiketrain": ["units", "copy", "sampling_rate", "t_start", "t_stop",
                   "waveforms", "left_sweep"],
    "event": ["times", "labels", "units", "durations", "copy"]
}


class NIXRawIO(BaseRawIO):

    extensions = ['nix', 'h5']
    rawmode = 'one-file'

    def __init__(self, filename='', block_index=0, autogenerate_stream_names=False, autogenerate_unit_ids=False):
        if autogenerate_stream_names:
            warnings.warn('Automatically generating streams based on signal order in files. '
                          'Potentially overwriting stream names.')
        if autogenerate_unit_ids:
            warnings.warn('Automatically generating unit_ids. Ignoring stored unit information '
                          'and using order of spiketrains instead.'
                          'Check for correct unit assignment.')
        check_nix_version()
        BaseRawIO.__init__(self)

        # checking consistency of generating neo version and autogeneration settings for reading
        import nixio
        nix_file = nixio.File.open(str(filename), nixio.FileMode.ReadOnly)
        neo_generation_version = Version(nix_file.sections['neo'].props['version'].values[0])
        if neo_generation_version < Version('0.7.0') and not autogenerate_stream_names:
            warnings.warn('Can load nix files generated by neo<0.7.0 only by autogenerating '
                          'stream names. Overwriting user setting `autogenerate_stream_names=False`')
            autogenerate_stream_names = True

        if neo_generation_version < Version('0.10.0') and not autogenerate_unit_ids:
            warnings.warn('Can load nix files generated by neo<0.7.0 only by autogenerating '
                          'unit ids. Overwriting user setting `autogenerate_unit_ids=False`')
            autogenerate_unit_ids = True

        print(f'{filename=}\t{neo_generation_version=}')



        self.filename = str(filename)
        self.autogenerate_stream_names = autogenerate_stream_names
        self.autogenerate_unit_ids = autogenerate_unit_ids
        self.block_index = block_index

    @staticmethod
    def get_block_count(filename):
        """
        Retrieve the number of Blocks present in the nix file

        Returns:
            (int) The number of blocks in the file.
        """
        import nixio
        nix_file = nixio.File.open(filename, nixio.FileMode.ReadOnly)
        block_count = len(nix_file.blocks)
        nix_file.close()

        return block_count

    def _source_name(self):
        return self.filename

    def _parse_header(self):
        import nixio

        self.file = nixio.File.open(self.filename, nixio.FileMode.ReadOnly)
        stream_name_by_id = {}

        self.nix_block = self.file.blocks[self.block_index]
        segment_groups = [g for g in self.nix_block.groups if g.type == "neo.segment"]
        neo_group_groups = [g for g in self.nix_block.groups if g.type == "neo.group"]

        def assert_channel_consistency(channels_by_segment):
            reference_channels = np.asarray(channels_by_segment[0])
            if not self.autogenerate_stream_names:
                # include name fields in stream comparison
                name_mask = list(reference_channels.dtype.fields.keys())
            else:
                # remove name fields from comparison
                name_mask = [k for k in reference_channels.dtype.fields.keys() if k != 'name']

            msg = 'Inconsistency across Segments: Try loading another block or use the ' \
                  'neo.io.nixio.NixIO for loading the nix file.'

            for segment_channels in channels_by_segment[1:]:
                # compare channel numbers
                if not len(segment_channels) == len(reference_channels):
                    raise ValueError(msg + f' Inconsistent number of channels.\n'
                                           f'{len(segment_channels)} != '
                                           f'{len(reference_channels)}')

                # compare channel details
                if not np.array_equal(segment_channels[name_mask], reference_channels[name_mask]):
                    raise ValueError(msg + f' Channels specifications are inconsistent:\n'
                                           f'{segment_channels} differs from '
                                           f'{reference_channels} ')

        def data_array_to_signal_channel(chan_id, stream_id, da):
            assert da.type == "neo.analogsignal"
            ch_name = da.metadata['neo_name']
            units = str(da.unit)
            dtype = str(da.dtype)
            # TODO: The sampling_interval unit is not taken into account for reading...
            sr = 1 / da.dimensions[0].sampling_interval
            gain = 1
            offset = 0.
            return (ch_name, chan_id, sr, dtype, units, gain, offset, stream_id)

        # construct signal channels for all segments
        segments_signal_channels = []
        for seg_idx, seg in enumerate(segment_groups):
            stream_id = -1
            last_anasig_id = 0
            signal_channels = []
            for da_idx, da in enumerate(seg.data_arrays):
                if da.type == "neo.analogsignal":
                    # identify stream_id by common anasig id
                    anasig_id = da.name.split('.')[-2]
                    stream_name = da.metadata.props['neo_name'].values[0]
                    if anasig_id != last_anasig_id:
                        stream_id += 1
                        last_anasig_id = anasig_id
                        if stream_id not in stream_name_by_id:
                            stream_name_by_id[stream_id] = stream_name

                    # sanity check for stream_id <=> stream names association

                    if not self.autogenerate_stream_names and stream_name_by_id[stream_id] != stream_name:
                        raise ValueError('Stream inconsistency across Segments or Blocks: '
                                         'Try loading individual blocks or use the '
                                         'neo.io.nixio.NixIO for loading the nix file.\n'
                                         f'{stream_id=} with {stream_name=} does not match '
                                         f'{stream_name_by_id=}.')

                    channel = data_array_to_signal_channel(da_idx, stream_id, da)
                    signal_channels.append(channel)

            signal_channels = np.asarray(signal_channels, dtype=_signal_channel_dtype)
            segments_signal_channels.append(signal_channels)

        # verify consistency across blocks
        assert_channel_consistency(segments_signal_channels)

        signal_streams = np.zeros(len(stream_name_by_id), dtype=_signal_stream_dtype)
        signal_streams['id'] = list(stream_name_by_id.keys())
        signal_streams['name'] = list(stream_name_by_id.values())

        def multi_tag_to_spike_channel(mt, unit_id):
            assert mt.type == "neo.spiketrain"
            unit_name = mt.metadata['neo_name']
            wf_left_sweep = 0
            wf_units = None
            wf_sampling_rate = 0
            if mt.features:
                wf = mt.features[0].data
                wf_units = wf.unit
                dim = wf.dimensions[2]
                interval = dim.sampling_interval
                wf_sampling_rate = 1 / interval
                if wf.metadata:
                    wf_left_sweep = wf.metadata["left_sweep"]
            wf_gain = 1
            wf_offset = 0.
            return (unit_name, unit_id, wf_units, wf_gain, wf_offset, wf_left_sweep, wf_sampling_rate)

        segments_spike_channels = []

        # detect neo groups that can be used to group spiketrains across segments
        neo_spiketrain_groups = []
        for group in neo_group_groups:
            # assume a group is a spiketrain `unit` when grouping only and as many spiketrains
            # as segments. Is there a better way to check for this?
            if (group.type == "neo.group" and
                    all([mt.type == "neo.spiketrain" for mt in group.multi_tags]) and
                    len(group.multi_tags) == len(segment_groups)):
                neo_spiketrain_groups.append(group)

        for seg in segment_groups:
            default_unit_id = 0
            spike_channels = []
            for mt in seg.multi_tags:
                if mt.type == "neo.spiketrain":

                    if self.autogenerate_unit_ids:
                        unit_id = default_unit_id

                    else:
                        # files generated with neo <0.10.0: extract or define unit id of spiketrain from nix sources
                        nix_generation_neo_version = Version(self.file.sections['neo'].props['version'].values[0])
                        if nix_generation_neo_version < Version('0.10.0'):
                            unit_sources = [s for s in mt.sources if s.type == 'neo.unit']

                            if len(unit_sources) == 1:
                                unit_id = unit_sources[0].name

                            elif len(unit_groups) == 0:
                                warnings.warn('No unit information found. Using default unit id.')
                                unit_id = default_unit_id

                            elif len(unit_sources) != 1:
                                raise ValueError('Ambiguous or missing unit assignment detected. '
                                                 'Use `autogenerate_unit_ids=True` to ignore '
                                                 'unit_ids in nix file and regenerate new ids.')


                        # files generated with recent neo versions use groups to groups spiketrains
                        elif nix_generation_neo_version >= Version('0.10.0'):
                            unit_groups = [g for g in neo_spiketrain_groups if mt in g.multi_tags]

                            if len(unit_groups) == 1:
                                unit_id = unit_groups[0].metadata.props['neo_name'].values[0]

                            elif len(unit_groups) == 0:
                                warnings.warn('No unit information found. Using default unit id.')
                                unit_id = default_unit_id

                            elif len(unit_groups) > 1:
                                raise ValueError('Ambiguous or missing unit assignment detected. '
                                                 'Use `autogenerate_unit_ids=True` to ignore '
                                                 'unit_ids in nix file and regenerate new ids.')

                    spike_channels.append(multi_tag_to_spike_channel(mt, unit_id))
                    default_unit_id += 1

            spike_channels = np.asarray(spike_channels, dtype=_spike_channel_dtype)
            segments_spike_channels.append(spike_channels)

        # verify consistency across segments
        assert_channel_consistency(segments_spike_channels)

        segments_event_channels = []
        for seg in segment_groups:
            event_count = 0
            epoch_count = 0
            event_channels = []
            for mt in seg.multi_tags:
                if mt.type == "neo.event":
                    ev_name = mt.metadata['neo_name']
                    ev_id = event_count
                    event_count += 1
                    ev_type = "event"
                    event_channels.append((ev_name, ev_id, ev_type))
                if mt.type == "neo.epoch":
                    ep_name = mt.metadata['neo_name']
                    ep_id = epoch_count
                    epoch_count += 1
                    ep_type = "epoch"
                    event_channels.append((ep_name, ep_id, ep_type))
            event_channels = np.asarray(event_channels, dtype=_event_channel_dtype)
            segments_event_channels.append(event_channels)

        assert_channel_consistency(segments_event_channels)

        # precollecting data array information
        self.da_list_by_segments = []
        for seg_index, seg in enumerate(segment_groups):
            d = {'signals': []}
            self.da_list_by_segments.append(d)
            size_list = []
            data_list = []
            da_name_list = []
            for da in seg.data_arrays:
                if da.type == 'neo.analogsignal':
                    size_list.append(da.size)
                    data_list.append(da)
                    da_name_list.append(da.metadata['neo_name'])
            d['data_size'] = size_list
            d['data'] = data_list
            d['ch_name'] = da_name_list

        self.unit_list_by_segment = []

        for seg_index, seg in enumerate(segment_groups):
            d = {'spiketrains': [],
                 'spiketrains_id': [],
                 'spiketrains_unit': [],
                 'waveforms': []}
            self.unit_list_by_segment.append(d)
            st_idx = 0
            for st in seg.multi_tags:
                if st.type == 'neo.spiketrain':
                    # populate spiketrain metadata & waveform
                    d['spiketrains'].append(st.positions)
                    d['spiketrains_id'].append(st.id)
                    d['spiketrains_unit'].append({'waveforms': []})
                    if st.features and st.features[0].data.type == "neo.waveforms":
                        waveforms = st.features[0].data
                        stdict = d['spiketrains_unit'][st_idx]
                        if waveforms:
                            stdict['waveforms'] = waveforms
                        else:
                            stdict['waveforms'] = None
                        # assume one spiketrain one waveform feature object
                        st_idx += 1

        self.header = {}
        self.header['nb_block'] = 1
        self.header['nb_segment'] = [len(segment_groups)]
        self.header['signal_streams'] = signal_streams
        # use signal, spike and event channels of first segments as these are consistent across segments
        self.header['signal_channels'] = segments_signal_channels[0]
        self.header['spike_channels'] = segments_spike_channels[0]
        self.header['event_channels'] = segments_event_channels[0]

        self._generate_minimal_annotations()
        bl_ann = self.raw_annotations['blocks'][0]
        props = self.nix_block.metadata.inherited_properties()
        bl_ann.update(self._filter_properties(props, "block"))
        for grp_idx, group in enumerate(segment_groups):
            seg_ann = bl_ann['segments'][grp_idx]
            props = group.metadata.inherited_properties()
            seg_ann.update(self._filter_properties(props, "segment"))

            sp_idx = 0
            ev_idx = 0
            for mt in group.multi_tags:
                if mt.type == 'neo.spiketrain' and seg_ann['spikes']:
                    st_ann = seg_ann['spikes'][sp_idx]
                    props = mt.metadata.inherited_properties()
                    st_ann.update(self._filter_properties(props, 'spiketrain'))
                    sp_idx += 1
                # if order is preserving, the annotations
                # should go to the right place, need test
                if mt.type == "neo.event" or mt.type == "neo.epoch":
                    if seg_ann['events'] != []:
                        event_ann = seg_ann['events'][ev_idx]
                        props = mt.metadata.inherited_properties()
                        event_ann.update(self._filter_properties(props, 'event'))
                        ev_idx += 1

                # adding array annotations to analogsignals
                annotated_anasigs = []
                sig_ann = seg_ann['signals']
                # this implementation relies on analogsignals always being
                # stored in the same stream order across segments
                stream_id = 0
                for da_idx, da in enumerate(group.data_arrays):
                    if da.type != "neo.analogsignal":
                        continue
                    anasig_id = da.name.split('.')[-2]
                    # skip already annotated signals as each channel already
                    # contains the complete set of annotations and
                    # array_annotations
                    if anasig_id in annotated_anasigs:
                        continue
                    annotated_anasigs.append(anasig_id)

                    # collect annotation properties
                    props = [p for p in da.metadata.props
                             if p.type != 'ARRAYANNOTATION']
                    props_dict = self._filter_properties(props, "analogsignal")
                    sig_ann[stream_id].update(props_dict)

                    # collect array annotation properties
                    props = [p for p in da.metadata.props
                             if p.type == 'ARRAYANNOTATION']
                    props_dict = self._filter_properties(props, "analogsignal")
                    sig_ann[stream_id]['__array_annotations__'].update(
                        props_dict)

                    stream_id += 1

    def _segment_t_start(self, block_index, seg_index):
        t_start = 0
        for mt in self.nix_block.groups[seg_index].multi_tags:
            if mt.type == "neo.spiketrain":
                t_start = mt.metadata['t_start']
        return t_start

    def _segment_t_stop(self, block_index, seg_index):
        t_stop = 0
        for mt in self.nix_block.groups[seg_index].multi_tags:
            if mt.type == "neo.spiketrain":
                t_stop = mt.metadata['t_stop']
        return t_stop

    def _get_signal_size(self, block_index, seg_index, stream_index):
        stream_id = self.header['signal_streams'][stream_index]['id']
        keep = self.header['signal_channels']['stream_id'] == stream_id
        channel_indexes, = np.nonzero(keep)
        ch_idx = channel_indexes[0]
        size = self.da_list_by_segments[seg_index]['data_size'][ch_idx]
        return size  # size is per signal, not the sum of all channel_indexes

    def _get_signal_t_start(self, block_index, seg_index, stream_index):
        stream_id = self.header['signal_streams'][stream_index]['id']
        keep = self.header['signal_channels']['stream_id'] == stream_id
        channel_indexes, = np.nonzero(keep)
        ch_idx = channel_indexes[0]
        das = [da for da in self.nix_block.groups[seg_index].data_arrays]
        da = das[ch_idx]
        sig_t_start = float(da.metadata['t_start'])
        return sig_t_start  # assume same group_id always same t_start

    def _get_analogsignal_chunk(self, block_index, seg_index, i_start, i_stop,
                                stream_index, channel_indexes):
        stream_id = self.header['signal_streams'][stream_index]['id']
        keep = self.header['signal_channels']['stream_id'] == stream_id
        global_channel_indexes, = np.nonzero(keep)
        if channel_indexes is not None:
            global_channel_indexes = global_channel_indexes[channel_indexes]

        if i_start is None:
            i_start = 0
        if i_stop is None:
            i_stop = self.get_signal_size(block_index, seg_index, stream_index)

        raw_signals_list = []
        da_list = self.da_list_by_segments[seg_index]
        for idx in global_channel_indexes:
            da = da_list['data'][idx]
            raw_signals_list.append(da[i_start:i_stop])

        raw_signals = np.array(raw_signals_list)
        raw_signals = np.transpose(raw_signals)
        return raw_signals

    def _spike_count(self, block_index, seg_index, unit_index):
        count = 0
        head_id = self.header['spike_channels'][unit_index][1]
        for mt in self.nix_block.groups[seg_index].multi_tags:
            for src in mt.sources:
                if mt.type == 'neo.spiketrain' and [src.type == "neo.unit"]:
                    if head_id == src.id:
                        return len(mt.positions)
        return count

    def _get_spike_timestamps(self, block_index, seg_index, unit_index,
                              t_start, t_stop):
        block = self.unit_list['blocks'][block_index]
        segment = block['segments'][seg_index]
        spike_dict = segment['spiketrains']
        spike_timestamps = spike_dict[unit_index]
        spike_timestamps = np.transpose(spike_timestamps)

        if t_start is not None or t_stop is not None:
            lim0 = t_start
            lim1 = t_stop
            mask = (spike_timestamps >= lim0) & (spike_timestamps <= lim1)
            spike_timestamps = spike_timestamps[mask]
        return spike_timestamps

    def _rescale_spike_timestamp(self, spike_timestamps, dtype):
        spike_times = spike_timestamps.astype(dtype)
        return spike_times

    def _get_spike_raw_waveforms(self, block_index, seg_index, unit_index,
                                 t_start, t_stop):
        # this must return a 3D numpy array (nb_spike, nb_channel, nb_sample)
        seg = self.unit_list['blocks'][block_index]['segments'][seg_index]
        waveforms = seg['spiketrains_unit'][unit_index]['waveforms']
        if not waveforms:
            return None
        raw_waveforms = np.array(waveforms)

        if t_start is not None:
            lim0 = t_start
            mask = (raw_waveforms >= lim0)
            # use nan to keep the shape
            raw_waveforms = np.where(mask, raw_waveforms, np.nan)
        if t_stop is not None:
            lim1 = t_stop
            mask = (raw_waveforms <= lim1)
            raw_waveforms = np.where(mask, raw_waveforms, np.nan)
        return raw_waveforms

    def _event_count(self, block_index, seg_index, event_channel_index):
        event_count = 0
        segment = self.nix_block.groups[seg_index]
        for event in segment.multi_tags:
            if event.type == 'neo.event' or event.type == 'neo.epoch':
                if event_count == event_channel_index:
                    return len(event.positions)
                else:
                    event_count += 1
        return event_count

    def _get_event_timestamps(self, block_index, seg_index,
                              event_channel_index, t_start, t_stop):
        timestamp = []
        labels = []
        durations = []
        if event_channel_index is None:
            raise IndexError
        segments = [g for g in self.nix_block.groups if g.type == 'neo.segment']
        for mt in segments[seg_index].multi_tags:
            if mt.type == "neo.event" or mt.type == "neo.epoch":
                labels.append(mt.positions.dimensions[0].labels)
                po = mt.positions
                if (po.type == "neo.event.times" or po.type == "neo.epoch.times"):
                    timestamp.append(po)
                channel = self.header['event_channels'][event_channel_index]
                if channel['type'] == b'epoch' and mt.extents:
                    if mt.extents.type == 'neo.epoch.durations':
                        durations.append(np.array(mt.extents))
                else:
                    durations.append(None)
        timestamp = timestamp[event_channel_index][:]
        timestamp = np.array(timestamp, dtype="float")
        durations = durations[event_channel_index]
        labels = labels[event_channel_index][:]
        labels = np.array(labels, dtype='U')
        if t_start is not None:
            keep = timestamp >= t_start
            timestamp, labels = timestamp[keep], labels[keep]
            if durations is not None:
                durations = durations[keep]

        if t_stop is not None:
            keep = timestamp <= t_stop
            timestamp, labels = timestamp[keep], labels[keep]
            if durations is not None:
                durations = durations[keep]
        return timestamp, durations, labels  # only the first fits in rescale

    def _rescale_event_timestamp(self, event_timestamps, dtype, event_channel_index):
        ev_unit = ''
        for mt in self.nix_block.groups[0].multi_tags:
            if mt.type == "neo.event":
                ev_unit = mt.positions.unit
                break
        if ev_unit == 'ms':
            event_timestamps /= 1000
        event_times = event_timestamps.astype(dtype)
        # supposing unit is second, other possibilities maybe mS microS...
        return event_times  # return in seconds

    def _rescale_epoch_duration(self, raw_duration, dtype, event_channel_index):
        ep_unit = ''
        for mt in self.nix_block.groups[0].multi_tags:
            if mt.type == "neo.epoch":
                ep_unit = mt.positions.unit
                break
        if ep_unit == 'ms':
            raw_duration /= 1000
        durations = raw_duration.astype(dtype)
        # supposing unit is second, other possibilities maybe mS microS...
        return durations  # return in seconds

    def _filter_properties(self, properties, neo_type):
        """
        Takes a collection of NIX metadata properties and the name of a Neo
        type and returns a dictionary representing the Neo object annotations.
        Properties that represent the attributes of the Neo object type are
        filtered, based on the global 'neo_attributes' dictionary.
        """
        annotations = dict()
        attrs = neo_attributes.get(neo_type, list())
        for prop in properties:
            # filter neo_name explicitly
            if not (prop.name in attrs or prop.name == "neo_name"):
                values = prop.values
                if len(values) == 1:
                    values = values[0]
                annotations[str(prop.name)] = values
        return annotations
